# -*- coding: utf-8 -*-
"""Deep_leaning_proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11GYPtjA12FCdQGDlDfuWTiHcYIovqM2t
"""

# =========================
# INSTALL LIBRARIES
# =========================
!pip -q install sentence-transformers pdfplumber

# =========================
# IMPORTS
# =========================
import pdfplumber
import re
import torch
from sentence_transformers import SentenceTransformer, util

# =========================
# LOAD MODEL
# =========================
print("Loading Transformer model...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("Model loaded\n")

# =========================
# READ PDF
# =========================
def read_pdf(path):
    text = ""
    with pdfplumber.open(path) as pdf:
        for page in pdf.pages:
            t = page.extract_text()
            if t:
                text += t + "\n"
    return text

# =========================
# CLEAN TEXT (FIX PDF ENCODING ISSUES)
# =========================
def clean_text(text):
    text = re.sub(r'\(cid:\d+\)', '', text)   # remove weird cid chars
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# =========================
# CREATE KNOWLEDGE CHUNKS
# Each ERROR + Solution together
# =========================
def create_knowledge(text):
    parts = re.split(r'ERROR\s+E\d+:', text)[1:]
    knowledge = []
    for p in parts:
        chunk = "ERROR " + p.strip()
        knowledge.append(chunk)
    return knowledge

# =========================
# LOAD YOUR PDF HERE
# =========================
pdf_path = "machine_manual.pdf"   # upload this file first

raw_text = read_pdf(pdf_path)
cleaned_text = clean_text(raw_text)
knowledge = create_knowledge(cleaned_text)

print("Knowledge entries found:", len(knowledge))

# =========================
# CREATE EMBEDDINGS DATABASE
# =========================
print("Creating embeddings...")
knowledge_embeddings = model.encode(knowledge, convert_to_tensor=True)
print("System Ready âœ…\n")

# =========================
# SEARCH FUNCTION
# =========================
def search_answer(query):
    query_embedding = model.encode(query, convert_to_tensor=True)
    scores = util.cos_sim(query_embedding, knowledge_embeddings)[0]
    best_idx = torch.argmax(scores)
    best_score = float(scores[best_idx])
    return knowledge[int(best_idx)], best_score

# =========================
# INTERACTIVE CHAT LOOP
# =========================
while True:
    q = input("Ask about machine error (or type exit): ")
    if q.lower() == "exit":
        break

    answer, score = search_answer(q)

    print("\nBest Match:\n", answer)
    print("Confidence:", round(score,3))
    print("-"*50)

# =========================
# DEMO SELF TEST ACCURACY
# =========================
print("\nRunning demo test...")

tests = [
    ("network issue", "network"),
    ("machine overheating", "overheating"),
    ("display problem", "display"),
    ("sensor not working", "sensor"),
]

correct = 0
for q, keyword in tests:
    result,_ = search_answer(q)
    if keyword.lower() in result.lower():
        correct += 1

accuracy = correct/len(tests)*100
print("Demo Accuracy:", accuracy,"%")

from google.colab import drive
drive.mount('/content/drive')